## PV / PVC
[deployment.yaml](../practice/5.saving-data/3.pvc/deployment.yaml)

```bash
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: fileshare  
```
- SC - StorageClass - хранит параметры подключения
- PVC - PersistentVolumeClaim - описывает требования к тому
- PV - PersistentVolume - хранит параметры и статус тома

Мы в деплойменте пишем не параметры тома, который мы хотим использовать, а ссылку на новый объект куба, 
который называется persistentVolumeClaim и говорим, какое у этого объекта имя - claimName: fileshare. 
И вот теперь в этом новом обхекте, мы описываем все параметр нашего диска, которые нам нужны. 
На самом деле главными параметрами для нас будут:
- размер диска
- из какой системы нам нужен диск
- тип доситупа к диску:
  - readWriteOnce - эксклюзивное чтение/запись диска
  - readWriteMany - один и тот же диск могут использовать несколько подов
  - readManyWriteOnce - читаюст все пишет один

Сам куб данные не хранит. Нам нужна какая-то система хранения данных. Это может быть железная полка, облака и т.п. 
В StorageClass мы описываем грубо говоря креды, при помощи которых куб будет взаимодействовать с этими 
системами хранения данных для того чтобы эти диски с полок подключать к нодам своего кластера. И потом точка 
монитрования будет прокидываться внутрь контейнера.

PersistentVolumeClaim - это наша заявка на диск, какой нам volume нужен. 

PersistentVolume - абстракция, в которую записывается инфа, какой диск был выдан нашему приложению. 
Типа id диска, адрес 

Откуда эти pv берутся? Например админ приходит и нарезает диски. Берет id этих дисков и создает манифесты PV внутри куба 

[11. pv_pvc.md](11.%20pv_pvc.md)

Создан набор дисков PV1, PV2, PV3. PV1, PV2 - у нас находятся StorageClass NFS, а PV3 в RBD. Т.е. это две раздельные 
системы хранения данных. У нас в кубе появляется PersistentVolumeClaim на 50Gb и требудет StorageClass: NFS.
Куб смотрит на манифесты, которые у него есть, те самые PV. Видит, что нам идеально подходит PV1. 


Кубер занимает этот диск (bound). Т.е. этот PV переводится в состояние BOUND связанные с нашим PersistentVolumeClaim 
и далее под, к котором был описан наш PVC, к нему подключается наш PV. Грубо говоря, та система хранения данных подключается 
к диску, нде был запущен наш под, данные прокидываются в контейнер, приложение начинает работать. Куб у себя внутри 
записывает, что 1 том был занят таким-то подом. Все больше никто его не может использовать, приложение его заняло и работает. 
у PV статус BOUND

Появляется второй claim и тож на 50Gb. Он займет PV на 100Gb, т.к. на 100 нас вполне себе устраивает. 
PV перейдет в статус BOUND и оставшиеся место не будет использоваться. 

Таким образом мы получсем неэффективное использованиве дискового хранилища. Читай далее - Provisioners

